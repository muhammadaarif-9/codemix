{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simAlign_exp_eng_hing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPYcm1DLHRY1fbfxJ1dV60d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muhammadaarif-9/codemix/blob/main/simAlign_metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "v7fuHT4-IfJ1"
      },
      "outputs": [],
      "source": [
        "#!pip install simalign\n",
        "#!pip install sacremoses\n",
        "from simalign import SentenceAligner\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# all_annotated\n",
        "data = []\n",
        "with open(\"all_annotated.txt\") as fp:\n",
        "  data = fp.read().split(\"\\n\")"
      ],
      "metadata": {
        "id": "BWhUVhsGLBpm"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# baseline_microsoft\n",
        "baseline_data = []\n",
        "with open(\"Microsoft_without_GA_858.txt\") as fp:\n",
        "  baseline_data = fp.read().split(\"\\n\")"
      ],
      "metadata": {
        "id": "G6bPGcnGOYgQ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store_data = []\n",
        "fp_store_data = open(\"recall.txt\",\"w\")\n",
        "def store(txt):\n",
        "  fp_store_data.write(txt+\"\\n\")\n",
        "models_count = 0\n",
        "models = [\"l3cube-pune/hing-mbert\",\"bert-base-multilingual-cased\",\"bert-base-multilingual-uncased\",\"xlm-mlm-100-1280\",\"roberta-base\",\"xlm-roberta-base\",\"xlm-roberta-large\",\"bert-base-uncased\"]"
      ],
      "metadata": {
        "id": "Q7-E0cXpCpAK"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "from typing import Dict, List, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "from scipy.stats import entropy\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "try:\n",
        "\timport networkx as nx\n",
        "\tfrom networkx.algorithms.bipartite.matrix import from_biadjacency_matrix\n",
        "except ImportError:\n",
        "\tnx = None\n",
        "import torch\n",
        "from transformers import BertModel, BertTokenizer, XLMModel, XLMTokenizer, RobertaModel, RobertaTokenizer, XLMRobertaModel, XLMRobertaTokenizer, AutoConfig, AutoModel, AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "from simalign.utils import get_logger\n",
        "\n",
        "LOG = get_logger(__name__)\n",
        "\n",
        "\n",
        "class EmbeddingLoader(object):\n",
        "\tdef __init__(self, model: str=\"xlm-roberta-base\", device=torch.device('cpu'), layer: int=8):\n",
        "\t\tTR_Models = {\n",
        "\t\t\t'bert-base-uncased': (BertModel, BertTokenizer),\n",
        "\t\t\t'bert-base-multilingual-cased': (BertModel, BertTokenizer),\n",
        "\t\t\t'bert-base-multilingual-uncased': (BertModel, BertTokenizer),\n",
        "\t\t\t'xlm-mlm-100-1280': (XLMModel, XLMTokenizer),\n",
        "\t\t\t'roberta-base': (RobertaModel, RobertaTokenizer),\n",
        "\t\t\t'xlm-roberta-base': (XLMRobertaModel, XLMRobertaTokenizer),\n",
        "\t\t\t'xlm-roberta-large': (XLMRobertaModel, XLMRobertaTokenizer),\n",
        "\t\t}\n",
        "\n",
        "\t\tself.model = model\n",
        "\t\tself.device = device\n",
        "\t\tself.layer = layer\n",
        "\t\tself.emb_model = None\n",
        "\t\tself.tokenizer = None\n",
        "\n",
        "\t\tif False:\n",
        "\t\t\tmodel_class, tokenizer_class = TR_Models[model]\n",
        "\t\t\tself.emb_model = AutoModelForMaskedLM.from_pretrained(model, output_hidden_states=True)\n",
        "\t\t\tself.emb_model.eval()\n",
        "\t\t\tself.emb_model.to(self.device)\n",
        "\t\t\tself.tokenizer = AutoTokenizer.from_pretrained(model,add_prefix_space=True)\n",
        "\t\telse:\n",
        "\t\t\t# try to load model with auto-classes\n",
        "\t\t\tconfig = AutoConfig.from_pretrained(model, output_hidden_states=True)\n",
        "\t\t\tself.emb_model = AutoModel.from_pretrained(model, config=config)\n",
        "\t\t\tself.emb_model.eval()\n",
        "\t\t\tself.emb_model.to(self.device)\n",
        "\t\t\tself.tokenizer = AutoTokenizer.from_pretrained(model,add_prefix_space=True) # add_prefix_space=True for roberta and gpt2\n",
        "\t\tLOG.info(\"Initialized the EmbeddingLoader with model: {}\".format(self.model))\n",
        "\n",
        "\tdef get_embed_list(self, sent_batch: List[List[str]]) -> torch.Tensor:\n",
        "\t\tif self.emb_model is not None:\n",
        "\t\t\twith torch.no_grad():\n",
        "\t\t\t\tif not isinstance(sent_batch[0], str):\n",
        "\t\t\t\t\tinputs = self.tokenizer(sent_batch, is_split_into_words=True, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tinputs = self.tokenizer(sent_batch, is_split_into_words=False, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\t\t\t\thidden = self.emb_model(**inputs.to(self.device))[\"hidden_states\"]\n",
        "\t\t\t\tif self.layer >= len(hidden):\n",
        "\t\t\t\t\traise ValueError(f\"Specified to take embeddings from layer {self.layer}, but model has only {len(hidden)} layers.\")\n",
        "\t\t\t\toutputs = hidden[self.layer]\n",
        "\t\t\t\treturn outputs[:, 1:-1, :]\n",
        "\t\telse:\n",
        "\t\t\treturn None\n",
        "\n",
        "\n",
        "class SentenceAligner(object):\n",
        "\tdef __init__(self, model: str = \"xlm-roberta-base\", token_type: str = \"bpe\", distortion: float = 0.0, matching_methods: str = \"mai\", device: str = \"cpu\", layer: int = 8):\n",
        "\t\tmodel_names = {\n",
        "\t\t\t\"bert\": \"bert-base-multilingual-cased\",\n",
        "\t\t\t\"xlmr\": \"xlm-roberta-base\"\n",
        "\t\t\t}\n",
        "\t\tall_matching_methods = {\"a\": \"inter\", \"m\": \"mwmf\", \"i\": \"itermax\", \"f\": \"fwd\", \"r\": \"rev\"}\n",
        "\n",
        "\t\tself.model = model\n",
        "\t\tif model in model_names:\n",
        "\t\t\tself.model = model_names[model]\n",
        "\t\tself.token_type = token_type\n",
        "\t\tself.distortion = distortion\n",
        "\t\tself.matching_methods = [all_matching_methods[m] for m in matching_methods]\n",
        "\t\tself.device = torch.device(device)\n",
        "\n",
        "\t\tself.embed_loader = EmbeddingLoader(model=self.model, device=self.device, layer=layer)\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef get_max_weight_match(sim: np.ndarray) -> np.ndarray:\n",
        "\t\tif nx is None:\n",
        "\t\t\traise ValueError(\"networkx must be installed to use match algorithm.\")\n",
        "\t\tdef permute(edge):\n",
        "\t\t\tif edge[0] < sim.shape[0]:\n",
        "\t\t\t\treturn edge[0], edge[1] - sim.shape[0]\n",
        "\t\t\telse:\n",
        "\t\t\t\treturn edge[1], edge[0] - sim.shape[0]\n",
        "\t\tG = from_biadjacency_matrix(csr_matrix(sim))\n",
        "\t\tmatching = nx.max_weight_matching(G, maxcardinality=True)\n",
        "\t\tmatching = [permute(x) for x in matching]\n",
        "\t\tmatching = sorted(matching, key=lambda x: x[0])\n",
        "\t\tres_matrix = np.zeros_like(sim)\n",
        "\t\tfor edge in matching:\n",
        "\t\t\tres_matrix[edge[0], edge[1]] = 1\n",
        "\t\treturn res_matrix\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef get_similarity(X: np.ndarray, Y: np.ndarray) -> np.ndarray:\n",
        "\t\treturn (cosine_similarity(X, Y) + 1.0) / 2.0\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef average_embeds_over_words(bpe_vectors: np.ndarray, word_tokens_pair: List[List[str]]) -> List[np.array]:\n",
        "\t\tw2b_map = []\n",
        "\t\tcnt = 0\n",
        "\t\tw2b_map.append([])\n",
        "\t\tfor wlist in word_tokens_pair[0]:\n",
        "\t\t\tw2b_map[0].append([])\n",
        "\t\t\tfor x in wlist:\n",
        "\t\t\t\tw2b_map[0][-1].append(cnt)\n",
        "\t\t\t\tcnt += 1\n",
        "\t\tcnt = 0\n",
        "\t\tw2b_map.append([])\n",
        "\t\tfor wlist in word_tokens_pair[1]:\n",
        "\t\t\tw2b_map[1].append([])\n",
        "\t\t\tfor x in wlist:\n",
        "\t\t\t\tw2b_map[1][-1].append(cnt)\n",
        "\t\t\t\tcnt += 1\n",
        "\n",
        "\t\tnew_vectors = []\n",
        "\t\tfor l_id in range(2):\n",
        "\t\t\tw_vector = []\n",
        "\t\t\tfor word_set in w2b_map[l_id]:\n",
        "\t\t\t\tw_vector.append(bpe_vectors[l_id][word_set].mean(0))\n",
        "\t\t\tnew_vectors.append(np.array(w_vector))\n",
        "\t\treturn new_vectors\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef get_alignment_matrix(sim_matrix: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "\t\tm, n = sim_matrix.shape\n",
        "\t\tforward = np.eye(n)[sim_matrix.argmax(axis=1)]  # m x n\n",
        "\t\tbackward = np.eye(m)[sim_matrix.argmax(axis=0)]  # n x m\n",
        "\t\treturn forward, backward.transpose()\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef apply_distortion(sim_matrix: np.ndarray, ratio: float = 0.5) -> np.ndarray:\n",
        "\t\tshape = sim_matrix.shape\n",
        "\t\tif (shape[0] < 2 or shape[1] < 2) or ratio == 0.0:\n",
        "\t\t\treturn sim_matrix\n",
        "\n",
        "\t\tpos_x = np.array([[y / float(shape[1] - 1) for y in range(shape[1])] for x in range(shape[0])])\n",
        "\t\tpos_y = np.array([[x / float(shape[0] - 1) for x in range(shape[0])] for y in range(shape[1])])\n",
        "\t\tdistortion_mask = 1.0 - ((pos_x - np.transpose(pos_y)) ** 2) * ratio\n",
        "\n",
        "\t\treturn np.multiply(sim_matrix, distortion_mask)\n",
        "\n",
        "\t@staticmethod\n",
        "\tdef iter_max(sim_matrix: np.ndarray, max_count: int=2) -> np.ndarray:\n",
        "\t\talpha_ratio = 0.9\n",
        "\t\tm, n = sim_matrix.shape\n",
        "\t\tforward = np.eye(n)[sim_matrix.argmax(axis=1)]  # m x n\n",
        "\t\tbackward = np.eye(m)[sim_matrix.argmax(axis=0)]  # n x m\n",
        "\t\tinter = forward * backward.transpose()\n",
        "\n",
        "\t\tif min(m, n) <= 2:\n",
        "\t\t\treturn inter\n",
        "\n",
        "\t\tnew_inter = np.zeros((m, n))\n",
        "\t\tcount = 1\n",
        "\t\twhile count < max_count:\n",
        "\t\t\tmask_x = 1.0 - np.tile(inter.sum(1)[:, np.newaxis], (1, n)).clip(0.0, 1.0)\n",
        "\t\t\tmask_y = 1.0 - np.tile(inter.sum(0)[np.newaxis, :], (m, 1)).clip(0.0, 1.0)\n",
        "\t\t\tmask = ((alpha_ratio * mask_x) + (alpha_ratio * mask_y)).clip(0.0, 1.0)\n",
        "\t\t\tmask_zeros = 1.0 - ((1.0 - mask_x) * (1.0 - mask_y))\n",
        "\t\t\tif mask_x.sum() < 1.0 or mask_y.sum() < 1.0:\n",
        "\t\t\t\tmask *= 0.0\n",
        "\t\t\t\tmask_zeros *= 0.0\n",
        "\n",
        "\t\t\tnew_sim = sim_matrix * mask\n",
        "\t\t\tfwd = np.eye(n)[new_sim.argmax(axis=1)] * mask_zeros\n",
        "\t\t\tbac = np.eye(m)[new_sim.argmax(axis=0)].transpose() * mask_zeros\n",
        "\t\t\tnew_inter = fwd * bac\n",
        "\n",
        "\t\t\tif np.array_equal(inter + new_inter, inter):\n",
        "\t\t\t\tbreak\n",
        "\t\t\tinter = inter + new_inter\n",
        "\t\t\tcount += 1\n",
        "\t\treturn inter\n",
        "\n",
        "\tdef get_word_aligns(self, src_sent: Union[str, List[str]], trg_sent: Union[str, List[str]]) -> Dict[str, List]:\n",
        "\t\tif isinstance(src_sent, str):\n",
        "\t\t\tsrc_sent = src_sent.split()\n",
        "\t\tif isinstance(trg_sent, str):\n",
        "\t\t\ttrg_sent = trg_sent.split()\n",
        "\t\tl1_tokens = [self.embed_loader.tokenizer.tokenize(word) for word in src_sent]\n",
        "\t\tl2_tokens = [self.embed_loader.tokenizer.tokenize(word) for word in trg_sent]\n",
        "\t\tbpe_lists = [[bpe for w in sent for bpe in w] for sent in [l1_tokens, l2_tokens]]\n",
        "\n",
        "\t\tif self.token_type == \"bpe\":\n",
        "\t\t\tl1_b2w_map = []\n",
        "\t\t\tfor i, wlist in enumerate(l1_tokens):\n",
        "\t\t\t\tl1_b2w_map += [i for x in wlist]\n",
        "\t\t\tl2_b2w_map = []\n",
        "\t\t\tfor i, wlist in enumerate(l2_tokens):\n",
        "\t\t\t\tl2_b2w_map += [i for x in wlist]\n",
        "\n",
        "\t\tvectors = self.embed_loader.get_embed_list([src_sent, trg_sent]).cpu().detach().numpy()\n",
        "\t\tvectors = [vectors[i, :len(bpe_lists[i])] for i in [0, 1]]\n",
        "\n",
        "\t\tif self.token_type == \"word\":\n",
        "\t\t\tvectors = self.average_embeds_over_words(vectors, [l1_tokens, l2_tokens])\n",
        "\n",
        "\t\tall_mats = {}\n",
        "\t\tsim = self.get_similarity(vectors[0], vectors[1])\n",
        "\t\tsim = self.apply_distortion(sim, self.distortion)\n",
        "\n",
        "\t\tall_mats[\"fwd\"], all_mats[\"rev\"] = self.get_alignment_matrix(sim)\n",
        "\t\tall_mats[\"inter\"] = all_mats[\"fwd\"] * all_mats[\"rev\"]\n",
        "\t\tif \"mwmf\" in self.matching_methods:\n",
        "\t\t\tall_mats[\"mwmf\"] = self.get_max_weight_match(sim)\n",
        "\t\tif \"itermax\" in self.matching_methods:\n",
        "\t\t\tall_mats[\"itermax\"] = self.iter_max(sim)\n",
        "\n",
        "\t\taligns = {x: set() for x in self.matching_methods}\n",
        "\t\tfor i in range(len(vectors[0])):\n",
        "\t\t\tfor j in range(len(vectors[1])):\n",
        "\t\t\t\tfor ext in self.matching_methods:\n",
        "\t\t\t\t\tif all_mats[ext][i, j] > 0:\n",
        "\t\t\t\t\t\tif self.token_type == \"bpe\":\n",
        "\t\t\t\t\t\t\taligns[ext].add((l1_b2w_map[i], l2_b2w_map[j]))\n",
        "\t\t\t\t\t\telse:\n",
        "\t\t\t\t\t\t\taligns[ext].add((i, j))\n",
        "\t\tfor ext in aligns:\n",
        "\t\t\taligns[ext] = sorted(aligns[ext])\n",
        "\t\treturn aligns"
      ],
      "metadata": {
        "id": "H-OK8qbqIxdN"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#simalign_model_name = models[models_count % len(models)]\n",
        "#models_count += 1"
      ],
      "metadata": {
        "id": "1qDxT9OEzuve"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#store(\"using \"+ simalign_model_name)"
      ],
      "metadata": {
        "id": "fFasUPb4Cndt"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(simalign_model_name)"
      ],
      "metadata": {
        "id": "SMBz05D80r4q"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# making an instance of our model.\n",
        "# You can specify the embedding model and all alignment settings in the constructor.\n",
        "#myaligner = SentenceAligner(model=\"xlm-roberta-base\", token_type=\"bpe\", matching_methods=\"maifr\") \n",
        "#myaligner = SentenceAligner(model=simalign_model_name, token_type=\"bpe\", matching_methods=\"maifr\") \n",
        "#myaligner = SentenceAligner(model=\"xlm-mlm-100-1280\", token_type=\"bpe\", matching_methods=\"maifr\") \n",
        "#myaligner = SentenceAligner(model=\"roberta-base\", token_type=\"bpe\", matching_methods=\"mai\") \n",
        "#myaligner = SentenceAligner(model=\"xlm-roberta-large\", token_type=\"bpe\", matching_methods=\"mai\")\n",
        "#myaligner = SentenceAligner(model=\"bert\", token_type=\"bpe\", matching_methods=\"mai\") "
      ],
      "metadata": {
        "id": "YbesBFs2JT3e"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #baseline\n",
        "# sent_num = 23\n",
        "# baseline_sent_num = 11\n",
        "# eng_sent = data[sent_num]\n",
        "# hing_sent = baseline_data[baseline_sent_num ]\n",
        "# print(\"sentence\")\n",
        "# print(eng_sent)\n",
        "# print(hing_sent)\n",
        "# eng_sent = word_tokenize(eng_sent)\n",
        "# hing_sent = word_tokenize(hing_sent)\n",
        "# print(\"After tokenize\")\n",
        "# print(eng_sent)\n",
        "# print(hing_sent)\n",
        "# print(\"after stopword removal\")\n",
        "# eng_sent =  [w for w in eng_sent if not w.lower() in stop_words]\n",
        "# hing_sent = [w for w in hing_sent if not w.lower() in stop_words]\n",
        "# print(eng_sent)\n",
        "# print(hing_sent)\n",
        "# # enumerate \n",
        "# print(list(enumerate(eng_sent)))\n",
        "# print(list(enumerate(hing_sent)))\n",
        "# groundtruth = [\n",
        "#   [(0,0),(1,1),(2,2)],\n",
        "#   [(0,0),(1,1),(2,2)],\n",
        "#   [(0,0),(1,1),(2,2)],\n",
        "#   [(0,1),(1,2),(2,3)],\n",
        "#   [(0,6),(1,4),(2,1)], \n",
        "# ]"
      ],
      "metadata": {
        "id": "iL2v3_usOvLY"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sent_num = 35\n",
        "# num_hing_sent = 8\n",
        "# eng_sent = data[sent_num]\n",
        "# hing_sent = data[36]\n",
        "# #hing_sent = baseline_data[11]\n",
        "# print(\"sentence\")\n",
        "# print(eng_sent)\n",
        "# print(hing_sent)\n",
        "# eng_sent = word_tokenize(eng_sent)\n",
        "# hing_sent = word_tokenize(hing_sent)\n",
        "# print(\"After tokenize\")\n",
        "# print(eng_sent)\n",
        "# print(hing_sent)\n",
        "# #print(\"after stopword removal\")\n",
        "# #eng_sent =  [w for w in eng_sent if not w.lower() in stop_words]\n",
        "# #hing_sent = [w for w in hing_sent if not w.lower() in stop_words]\n",
        "# #print(eng_sent)\n",
        "# #print(hing_sent)\n",
        "# # enumerate \n",
        "# print(list(enumerate(eng_sent)))\n",
        "# print(list(enumerate(hing_sent)))\n",
        "# groundtruth = [\n",
        "#     [(1,5),(3,3),(5,1)]\n",
        "# ]"
      ],
      "metadata": {
        "id": "eUUZvhPQwK0F"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # test\n",
        "# models_count+=1\n",
        "# simalign_model_name = models[models_count % len(models)]\n",
        "# myaligner = SentenceAligner(model=simalign_model_name, token_type=\"bpe\", matching_methods=\"maifr\")"
      ],
      "metadata": {
        "id": "heZX1x62O0Yc"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # test\n",
        "# alignments_simalign = myaligner.get_word_aligns(eng_sent, hing_sent)\n",
        "# count = 0\n",
        "# for matching_method in alignments_simalign:\n",
        "#   print(matching_method,alignments_simalign[matching_method])\n",
        "#   if len(groundtruth[0][0]) == 0:\n",
        "#     print(groundtruth[1]) \n",
        "#   for item in alignments_simalign[matching_method]:\n",
        "#     if item in groundtruth[0]:\n",
        "#       count+=1\n",
        "#   print(\"accuracy: \",count / len(groundtruth[0]))\n",
        "#   count = 0"
      ],
      "metadata": {
        "id": "8vWyZNRYPN6n"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "sent_metrics = []\n",
        "skip_track = 0\n",
        "for s in range(len(models)): # \n",
        "  simalign_model_name = models[models_count % len(models)]\n",
        "  models_count += 1\n",
        "  store(\"using \"+ simalign_model_name)\n",
        "  myaligner = SentenceAligner(model=simalign_model_name, token_type=\"bpe\", matching_methods=\"maifr\")\n",
        "  metric_score_per_sent_pair = []\n",
        "  for k in range(6,7):\n",
        "    if k == 0:\n",
        "      sent_num = 23\n",
        "      num_hing_sent = 8\n",
        "      store(\"############## [hinglish from human_annotated] ##############\")\n",
        "      store(\"sentence_pair 3\")\n",
        "      groundtruth = [\n",
        "        [(5,5),(6,6)],\n",
        "        [()],  # no mapping\n",
        "        [(6,8)],\n",
        "        [(5,5)],\n",
        "        [(6,6)],\n",
        "        [()], # no mapping\n",
        "        [(5,4),(6,5)],\n",
        "        [(5,7)]\n",
        "      ]\n",
        "    elif k == 1:\n",
        "      sent_num = 35\n",
        "      num_hing_sent = 8\n",
        "      store(\"sentence_pair 4\")\n",
        "      groundtruth = [\n",
        "        [(1,5)],\n",
        "        [()],  # no mapping\n",
        "        [(3,2)],\n",
        "        [(3,2)],\n",
        "        [(3,3)],\n",
        "        [()], # no mapping\n",
        "        [()],\n",
        "        [(1,6)]\n",
        "      ]\n",
        "    elif k == 2:\n",
        "      sent_num = 56\n",
        "      num_hing_sent = 7\n",
        "      store(\"sentence_pair 6\")\n",
        "      groundtruth = [\n",
        "        [(0,5)],\n",
        "        [(4,3)],  # no mapping\n",
        "        [(4,0)],\n",
        "        [(4,0)],\n",
        "        [(4,3)],\n",
        "        [()], # no mapping\n",
        "        [(0,4),(4,0)],\n",
        "      ]\n",
        "    elif k == 3:\n",
        "      sent_num = 77\n",
        "      num_hing_sent = 6\n",
        "      store(\"sentence_pair 8\")\n",
        "      groundtruth = [\n",
        "       [(4,1)],\n",
        "       [(5,2)],  # no mapping\n",
        "       [(4,1)],\n",
        "       [(5,2)],\n",
        "       [(4,4),(5,5)],\n",
        "       [()], # no mapping\n",
        "      ]\n",
        "    elif k == 4:\n",
        "      sent_num = 23\n",
        "      num_hing_sent = 2\n",
        "      store(\"############## [hinglish from Baseline] ##############\")\n",
        "      store(\"sentence_pair 3\")\n",
        "      groundtruth = [\n",
        "        [()],\n",
        "        [(6,7)]\n",
        "      ]\n",
        "      hing_sent_num = 3344\n",
        "    elif k == 5:\n",
        "      sent_num = 35\n",
        "      num_hing_sent = 5\n",
        "      store(\"sentence_pair 4\")\n",
        "      groundtruth = [\n",
        "        [()],\n",
        "        [(3,3),(5,6)],\n",
        "        [()],\n",
        "        [(3,3)],\n",
        "        [(1,6),(3,4)]\n",
        "      ]\n",
        "      hing_sent_num = 6\n",
        "    elif k == 6:\n",
        "      sent_num = 23\n",
        "      num_hing_sent = 1\n",
        "      store(\"############## [hindi from human annotation] ##############\")\n",
        "      store(\"sentence_pair 3\")\n",
        "      groundtruth = [\n",
        "        [(0,2),(3,0),(5,7),(6,8)]\n",
        "      ]\n",
        "    elif k == 7:\n",
        "      sent_num = 35\n",
        "      num_hing_sent = 1\n",
        "      store(\"sentence_pair 4\")\n",
        "      groundtruth = [\n",
        "        [(1,5),(3,3),(5,1)]\n",
        "      ]\n",
        "    eng_sent_num = sent_num\n",
        "    eng_sent = word_tokenize(data[sent_num])\n",
        "    sent_num += 2\n",
        "    if k >= 4 and k < 6: sent_num = hing_sent_num\n",
        "    if k >= 6: sent_num = sent_num - 1 \n",
        "  \n",
        "    metric_val_per_hing = []\n",
        "    for i in range(num_hing_sent):\n",
        "      skip_track = 0\n",
        "      if len(groundtruth[i][0]) == 0:\n",
        "        store(\"No non-trivial mappings ---> Skipping this sentence\")\n",
        "        skip_track = 1\n",
        "        continue\n",
        "      store(\"eng-hing sentence pair - hing sentence_num \"+str(i+1))\n",
        "      store(\"[eng] \"+ data[eng_sent_num])\n",
        "      if k >= 4 and k < 6:\n",
        "        store(\"[hing] \"+baseline_data[sent_num])\n",
        "      else:\n",
        "        store(\"[hing] \"+data[sent_num])\n",
        "      #tokenize\n",
        "      store(\"[nltk_tokenize]\")\n",
        "      store(\"[eng] \"+ str(eng_sent))\n",
        "      if k >= 4 and k < 6:\n",
        "        hing_sent = word_tokenize(baseline_data[sent_num])\n",
        "        # print(\"hinglish\",k)\n",
        "        # print(\"num_hing_sent\",num_hing_sent)\n",
        "        # print(\"count of hing\",i+1)\n",
        "      else:\n",
        "        hing_sent = word_tokenize(data[sent_num])\n",
        "      store(\"[hing] \"+ str(hing_sent))\n",
        "      #eng_sent = [w for w in eng_sent if not w.lower() in stop_words]\n",
        "      #hing_sent = [w for w in hing_sent if not w.lower() in stop_words]\n",
        "      #store(\"[after stopwords removal]\")\n",
        "      #store(\"[eng]\"+str(eng_sent))\n",
        "      #store(\"[hing]\"+str(hing_sent))\n",
        "      store(\"[enumerate tokens]\")\n",
        "      store(\"[eng]\"+str(list(enumerate(eng_sent))))\n",
        "      store(\"[hing]\"+str(list(enumerate(hing_sent))))\n",
        "      store(\"[groundtruth]\")\n",
        "      store(str(groundtruth[i]))\n",
        "      alignments_simalign = myaligner.get_word_aligns(eng_sent, hing_sent)\n",
        "      metric_val_per_method = []\n",
        "      count = 0\n",
        "      for matching_method in alignments_simalign:\n",
        "        store(\"method: \"+matching_method)\n",
        "        store(\"sent_num: \"+ str(sent_num))\n",
        "        align_map = []\n",
        "        for item in alignments_simalign[matching_method]:\n",
        "          if item in groundtruth[i]:\n",
        "            temp = str(eng_sent[item[0]]) + \"----->\" + str(hing_sent[item[1]])\n",
        "            align_map.append(temp)\n",
        "            count+=1\n",
        "        store(\"mappings:\"+str(align_map))\n",
        "        store(\"recall: \"+str(count / len(groundtruth[i])))\n",
        "        metric_val_per_method.append(str(count / len(groundtruth[i])))\n",
        "        count = 0\n",
        "      metric_val_per_hing.append(metric_val_per_method)\n",
        "      sent_num += 1\n",
        "      store(\"\\n\")\n",
        "    metric_score_all_hing_pair = []\n",
        "    for loop_iter_row in range(len(metric_val_per_hing[0])):\n",
        "      metric_score_all_hing_pair.append(0)\n",
        "\n",
        "    for loop_iter_row in range(len(metric_val_per_hing)):\n",
        "      for loop_iter_col in range(len(metric_val_per_hing[loop_iter_row])):\n",
        "        metric_score_all_hing_pair[loop_iter_col] = float(metric_score_all_hing_pair[loop_iter_col]) +  float(metric_val_per_hing[loop_iter_row][loop_iter_col])\n",
        "    \n",
        "    for loop_iter_row in range(len(metric_val_per_hing[0])):\n",
        "      metric_score_all_hing_pair[loop_iter_row] = float(metric_score_all_hing_pair[loop_iter_row]) / float(len(metric_val_per_hing))\n",
        "    \n",
        "    metric_score_per_sent_pair.append(metric_score_all_hing_pair)\n",
        "  \n",
        "  temp_list = []\n",
        "  for loop_iter_row in range(len(metric_score_per_sent_pair[0])):\n",
        "    temp_list.append(0)\n",
        "  for loop_iter_row in range(len(metric_score_per_sent_pair)):\n",
        "    for loop_iter_col in range(len(metric_score_per_sent_pair[loop_iter_row])):\n",
        "      temp_list[loop_iter_col] = float(temp_list[loop_iter_col]) +  float(metric_score_per_sent_pair[loop_iter_row][loop_iter_col])\n",
        "  for loop_iter_row in range(len(metric_score_per_sent_pair[0])):\n",
        "    temp_list[loop_iter_row] = float(temp_list[loop_iter_row]) / float(len(metric_score_per_sent_pair))\n",
        "  temp_list.insert(0,simalign_model_name)\n",
        "  sent_metrics.append(temp_list)  \n",
        "fp_store_data.close()"
      ],
      "metadata": {
        "id": "nv9SaBa4ZRQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_row = [\"Model Name\",\"mwmf\",\"inter\",\"itermax\",\"fwd\",\"rev\"]\n",
        "table = [first_row,sent_metrics[0],\n",
        "         sent_metrics[1],\n",
        "         sent_metrics[2],\n",
        "         sent_metrics[3],\n",
        "         sent_metrics[4],\n",
        "         sent_metrics[5],\n",
        "         sent_metrics[6],\n",
        "         sent_metrics[7]]"
      ],
      "metadata": {
        "id": "pCApT2T4aNLM"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install tabulate\n",
        "from tabulate import tabulate\n",
        "print(\"####################### Recall metrics for 4-Sentence Pair [eng-hing]####################\")\n",
        "print(tabulate(table,headers='firstrow',tablefmt='grid',stralign='center',showindex=\"always\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1yRPr8ANtvl",
        "outputId": "a432417c-dd71-4efd-cc5e-3f54bd86e085"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "####################### Recall metrics for 4-Sentence Pair [eng-hing]####################\n",
            "+----+--------------------------------+--------+---------+-----------+-------+-------+\n",
            "|    |           Model Name           |   mwmf |   inter |   itermax |   fwd |   rev |\n",
            "+====+================================+========+=========+===========+=======+=======+\n",
            "|  0 |     l3cube-pune/hing-mbert     |   0.5  |    0    |      0    |  0.25 |  0.25 |\n",
            "+----+--------------------------------+--------+---------+-----------+-------+-------+\n",
            "|  1 |  bert-base-multilingual-cased  |   0.75 |    0.5  |      0.5  |  0.5  |  0.5  |\n",
            "+----+--------------------------------+--------+---------+-----------+-------+-------+\n",
            "|  2 | bert-base-multilingual-uncased |   0.5  |    0    |      0.25 |  0.25 |  0.5  |\n",
            "+----+--------------------------------+--------+---------+-----------+-------+-------+\n",
            "|  3 |        xlm-mlm-100-1280        |   0.25 |    0.25 |      0.25 |  0.5  |  0.5  |\n",
            "+----+--------------------------------+--------+---------+-----------+-------+-------+\n",
            "|  4 |          roberta-base          |   0.25 |    0    |      0    |  0    |  0.75 |\n",
            "+----+--------------------------------+--------+---------+-----------+-------+-------+\n",
            "|  5 |        xlm-roberta-base        |   1    |    1    |      1    |  1    |  1    |\n",
            "+----+--------------------------------+--------+---------+-----------+-------+-------+\n",
            "|  6 |       xlm-roberta-large        |   1    |    0.75 |      0.75 |  0.75 |  1    |\n",
            "+----+--------------------------------+--------+---------+-----------+-------+-------+\n",
            "|  7 |       bert-base-uncased        |   0.5  |    0    |      0    |  0.25 |  0.5  |\n",
            "+----+--------------------------------+--------+---------+-----------+-------+-------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from IPython.display import display\n",
        "# import pandas as pd\n",
        "# dataframe = pd.DataFrame(all_stats)\n",
        "# # displaying the DataFrame\n",
        "# display(dataframe)"
      ],
      "metadata": {
        "id": "ETUAmqfTYiB_"
      },
      "execution_count": 493,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# previous one\n",
        "# for s in range(len(models)): # \n",
        "#   simalign_model_name = models[models_count % len(models)]\n",
        "#   models_count += 1\n",
        "#   store(\"using \"+ simalign_model_name)\n",
        "#   print(simalign_model_name)\n",
        "#   myaligner = SentenceAligner(model=simalign_model_name, token_type=\"bpe\", matching_methods=\"maifr\")\n",
        "#   for k in range(1):\n",
        "#     if k == 0:\n",
        "#       sent_num = 23\n",
        "#       num_hing_sent = 8\n",
        "#       store(\"############## [hinglish from human_annotated] ##############\")\n",
        "#       store(\"sentence_pair 3\")\n",
        "#       groundtruth = [\n",
        "#         [(5,5),(6,6)],\n",
        "#         [()],  # no mapping\n",
        "#         [(6,8)],\n",
        "#         [(5,5)],\n",
        "#         [(6,6)],\n",
        "#         [()], # no mapping\n",
        "#         [(5,4),(6,5)],\n",
        "#         [(5,7)]\n",
        "#       ]\n",
        "#     elif k == 1:\n",
        "#       sent_num = 35\n",
        "#       num_hing_sent = 8\n",
        "#       store(\"sentence_pair 4\")\n",
        "#       groundtruth = [\n",
        "#         [(0,4),(1,1),(2,2),(3,6)],\n",
        "#         [(0,3),(1,1),(2,2),(3,5)],\n",
        "#         [(0,0),(1,1),(2,2),(3,3)],\n",
        "#         [(0,5),(1,2),(2,0),(3,7)],\n",
        "#         [(0,5),(1,3),(2,1),(3,7)],\n",
        "#         [(0,0),(1,3),(2,1),(3,4)],\n",
        "#         [(0,0),(1,1),(2,2),(3,3)],\n",
        "#         [(0,4),(1,2),(2,0),(3,5)]\n",
        "#       ]\n",
        "#     elif k == 2:\n",
        "#       sent_num = 56\n",
        "#       num_hing_sent = 7\n",
        "#       store(\"sentence_pair 6\")\n",
        "#       groundtruth = [\n",
        "#         [(0,5),(1,2),(2,0),(3,1)],\n",
        "#         [(0,0),(1,1),(3,2)],\n",
        "#         [(0,4),(1,1),(3,0)],\n",
        "#         [(0,4),(1,2),(3,0)],\n",
        "#         [(0,0),(1,1),(3,2)],\n",
        "#         [(0,4),(1,2),(2,0),(3,1),(4,5)],\n",
        "#         [(0,3),(1,1),(3,0),(4,4)]\n",
        "#       ]\n",
        "#     elif k == 3:\n",
        "#       sent_num = 77\n",
        "#       num_hing_sent = 6\n",
        "#       store(\"sentence_pair 8\")\n",
        "#       groundtruth = [\n",
        "#         [(0,3),(1,4),(2,1),(3,2)],\n",
        "#         [(0,2),(1,3),(2,0),(3,1)],\n",
        "#         [(0,2),(1,3),(2,0),(3,1)],\n",
        "#         [(0,3),(1,4),(2,1),(3,2)],\n",
        "#         [(0,0),(1,1),(2,2),(3,3)],\n",
        "#         [(0,0),(1,1),(2,3),(3,4)],\n",
        "#       ]\n",
        "#     elif k == 4:\n",
        "#       sent_num = 23\n",
        "#       num_hing_sent = 2\n",
        "#       store(\"############## [hinglish from Baseline] ##############\")\n",
        "#       store(\"sentence_pair 3\")\n",
        "#       groundtruth = [\n",
        "#         [(0,2),(1,0),(2,5),(3,6),(4,7)],\n",
        "#         [(0,2),(1,0),(2,5),(3,6),(4,7)]\n",
        "#       ]\n",
        "#       hing_sent_num = 3344\n",
        "#     elif k == 5:\n",
        "#       sent_num = 35\n",
        "#       num_hing_sent = 5\n",
        "#       store(\"sentence_pair 4\")\n",
        "#       groundtruth = [\n",
        "#         [(0,0),(1,1),(2,2)],\n",
        "#         [(0,0),(1,1),(2,2)],\n",
        "#         [(0,0),(1,1),(2,2)],\n",
        "#         [(0,1),(1,2),(2,3)],\n",
        "#         [(0,6),(1,4),(2,1)] \n",
        "#       ]\n",
        "#       hing_sent_num = 6\n",
        "#     elif k == 6:\n",
        "#       sent_num = 23\n",
        "#       num_hing_sent = 1\n",
        "#       store(\"############## [hindi from human annotation] ##############\")\n",
        "#       store(\"sentence_pair 3\")\n",
        "#       groundtruth = [\n",
        "#         [(0,2),(1,0),(2,6),(3,7),(4,8),(5,10)]\n",
        "#       ]\n",
        "#     elif k == 7:\n",
        "#       sent_num = 35\n",
        "#       num_hing_sent = 1\n",
        "#       store(\"sentence_pair 4\")\n",
        "#       groundtruth = [\n",
        "#         [(0,5),(1,3),(2,1)]\n",
        "#       ]\n",
        "#     eng_sent_num = sent_num\n",
        "#     eng_sent = word_tokenize(data[sent_num])\n",
        "#     sent_num += 2\n",
        "#     if k >= 4 and k < 6: sent_num = hing_sent_num\n",
        "#     if k >= 6: sent_num = sent_num - 1 \n",
        "#     for i in range(num_hing_sent):\n",
        "#       store(\"eng-hing sentence pair - hing sentence_num \"+str(i+1))\n",
        "#       store(\"[eng] \"+ data[eng_sent_num])\n",
        "#       if k >= 4 and k < 6:\n",
        "#         store(\"[hing] \"+baseline_data[sent_num])\n",
        "#       else:\n",
        "#         store(\"[hing] \"+data[sent_num])\n",
        "#       #tokenize\n",
        "#       store(\"[nltk_tokenize]\")\n",
        "#       store(\"[eng] \"+ str(eng_sent))\n",
        "#       if k >= 4 and k < 6:\n",
        "#         hing_sent = word_tokenize(baseline_data[sent_num])\n",
        "#         # print(\"hinglish\",k)\n",
        "#         # print(\"num_hing_sent\",num_hing_sent)\n",
        "#         # print(\"count of hing\",i+1)\n",
        "#       else:\n",
        "#         hing_sent = word_tokenize(data[sent_num])\n",
        "#       store(\"[hing] \"+ str(hing_sent))\n",
        "#       eng_sent = [w for w in eng_sent if not w.lower() in stop_words]\n",
        "#       hing_sent = [w for w in hing_sent if not w.lower() in stop_words]\n",
        "#       store(\"[after stopwords removal]\")\n",
        "#       store(\"[eng]\"+str(eng_sent))\n",
        "#       store(\"[hing]\"+str(hing_sent))\n",
        "#       store(\"[enumerate tokens]\")\n",
        "#       store(\"[eng]\"+str(list(enumerate(eng_sent))))\n",
        "#       store(\"[hing]\"+str(list(enumerate(hing_sent))))\n",
        "#       store(\"[groundtruth]\")\n",
        "#       store(str(groundtruth[i]))\n",
        "#       alignments_simalign = myaligner.get_word_aligns(eng_sent, hing_sent)\n",
        "#       store_alignments = {}\n",
        "#       for matching_method in alignments_simalign:\n",
        "#         store_alignments[matching_method] = alignments_simalign[matching_method]\n",
        "#       store(\"[simalign output]\")\n",
        "#       store(str(store_alignments))\n",
        "#       methods_of_simalign = [\"mwmf\",\"inter\",\"itermax\",\"fwd\",\"rev\"]\n",
        "#       for j in range(len(methods_of_simalign)):\n",
        "#         simalign_method = methods_of_simalign[j]\n",
        "#         count = 0\n",
        "#         simalign_map = []\n",
        "#         for item in alignments_simalign[simalign_method]:\n",
        "#           temp = str(eng_sent[item[0]]) + \"----->\" + str(hing_sent[item[1]])\n",
        "#           simalign_map.append(temp)\n",
        "#           if item in groundtruth[i]:\n",
        "#             count += 1\n",
        "#         # accuracy \n",
        "#         accuracy = round(count/len(alignments_simalign[simalign_method]),2)\n",
        "#         store(\"method: \"+simalign_method)\n",
        "#         store(str(simalign_map))\n",
        "#         store(\"accuracy: \"+str(accuracy))\n",
        "#       sent_num += 1\n",
        "#       store(\"\\n\")\n",
        "# fp_store_data.close()"
      ],
      "metadata": {
        "id": "xkW95gow7YHs"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0U1H3_GuYrKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for k in range(4):\n",
        "#   if k == 0:\n",
        "#     sent_num = 23\n",
        "#     num_hing_sent = 8\n",
        "#     store(\"sentence_pair 3\")\n",
        "#     groundtruth = [\n",
        "#       [(0,0),(1,1),(2,2),(3,3),(4,4),(5,6)],\n",
        "#       [(0,2),(1,0),(2,6),(3,7),(4,8),(5,9)],\n",
        "#       [(0,2),(1,0),(2,6),(3,7),(4,8),(5,10)],\n",
        "#       [(0,0),(1,1),(2,2),(3,3),(4,4),(5,6)],\n",
        "#       [(0,0),(1,1),(2,2),(3,3),(4,4),(5,6)],\n",
        "#       [(0,2),(1,0),(2,6),(3,7),(4,8),(5,9)],\n",
        "#       [(0,0),(1,1),(2,2),(3,3),(4,4),(5,6)],\n",
        "#       [(0,2),(1,0),(2,6),(3,7),(4,8),(5,10)]\n",
        "#     ]\n",
        "#   elif k == 1:\n",
        "#     sent_num = 35\n",
        "#     num_hing_sent = 8\n",
        "#     store(\"sentence_pair 4\")\n",
        "#     groundtruth = [\n",
        "#       [(0,4),(1,1),(2,2),(3,6)],\n",
        "#       [(0,3),(1,1),(2,2),(3,5)],\n",
        "#       [(0,0),(1,1),(2,2),(3,3)],\n",
        "#       [(0,5),(1,2),(2,0),(3,7)],\n",
        "#       [(0,5),(1,3),(2,1),(3,7)],\n",
        "#       [(0,0),(1,3),(2,1),(3,4)],\n",
        "#       [(0,0),(1,1),(2,2),(3,3)],\n",
        "#       [(0,4),(1,2),(2,0),(3,5)]\n",
        "#     ]\n",
        "#   elif k == 2:\n",
        "#     sent_num = 56\n",
        "#     num_hing_sent = 7\n",
        "#     store(\"sentence_pair 6\")\n",
        "#     groundtruth = [\n",
        "#       [(0,5),(1,2),(2,0),(3,1)],\n",
        "#       [(0,0),(1,1),(3,2)],\n",
        "#       [(0,4),(1,1),(3,0)],\n",
        "#       [(0,4),(1,2),(3,0)],\n",
        "#       [(0,0),(1,1),(3,2)],\n",
        "#       [(0,4),(1,2),(2,0),(3,1),(4,5)],\n",
        "#       [(0,3),(1,1),(3,0),(4,4)]\n",
        "#     ]\n",
        "#   elif k == 3:\n",
        "#     sent_num = 77\n",
        "#     num_hing_sent = 6\n",
        "#     store(\"sentence_pair 8\")\n",
        "#     groundtruth = [\n",
        "#       [(0,3),(1,4),(2,1),(3,2)],\n",
        "#       [(0,2),(1,3),(2,0),(3,1)],\n",
        "#       [(0,2),(1,3),(2,0),(3,1)],\n",
        "#       [(0,3),(1,4),(2,1),(3,2)],\n",
        "#       [(0,0),(1,1),(2,2),(3,3)],\n",
        "#       [(0,0),(1,1),(2,3),(3,4)],\n",
        "#     ]\n",
        "#   store(\"[eng] \"+ data[sent_num])\n",
        "#   eng_sent = word_tokenize(data[sent_num])\n",
        "#   sent_num += 2\n",
        "#   for i in range(num_hing_sent):\n",
        "#     store(\"[hing] \"+data[sent_num])\n",
        "#     #tokenize\n",
        "#     store(\"[nltk_tokenize]\")\n",
        "#     store(\"[eng] \"+ str(eng_sent))\n",
        "#     hing_sent = word_tokenize(data[sent_num])\n",
        "#     store(\"[hing] \"+ str(hing_sent))\n",
        "#     eng_sent = [w for w in eng_sent if not w.lower() in stop_words]\n",
        "#     hing_sent = [w for w in hing_sent if not w.lower() in stop_words]\n",
        "#     store(\"after stopwords removal\")\n",
        "#     store(\"[eng]\"+str(eng_sent))\n",
        "#     store(\"[hing]\"+str(hing_sent))\n",
        "#     store(\"enumerate tokens\")\n",
        "#     store(\"[eng]\"+str(list(enumerate(eng_sent))))\n",
        "#     store(\"[hing]\"+str(list(enumerate(hing_sent))))\n",
        "#     store(\"groundtruth\")\n",
        "#     store(str(groundtruth[i]))\n",
        "#     alignments_simalign = myaligner.get_word_aligns(eng_sent, hing_sent)\n",
        "#     store_alignments = {}\n",
        "#     for matching_method in alignments_simalign:\n",
        "#       store_alignments[matching_method] = alignments_simalign[matching_method]\n",
        "#     store(\"simalign output\")\n",
        "#     store(str(store_alignments))\n",
        "#     methods_of_simalign = [\"mwmf\",\"inter\",\"itermax\",\"fwd\",\"rev\"]\n",
        "#     for j in range(len(methods_of_simalign)):\n",
        "#       simalign_method = methods_of_simalign[j]\n",
        "#       count = 0\n",
        "#       simalign_map = []\n",
        "#       for item in alignments_simalign[simalign_method]:\n",
        "#         temp = str(eng_sent[item[0]]) + \"----->\" + str(hing_sent[item[1]])\n",
        "#         simalign_map.append(temp)\n",
        "#         if item in groundtruth[i]:\n",
        "#           count += 1\n",
        "#       # accuracy \n",
        "#       accuracy = round(count/len(alignments_simalign[simalign_method]),2)\n",
        "#       store(\"method: \"+simalign_method)\n",
        "#       store(\"accuracy: \"+str(accuracy))\n",
        "#       store(str(simalign_map))\n",
        "#     sent_num += 1\n",
        "#     store(\"\\n\")"
      ],
      "metadata": {
        "id": "feZP0Zu63zP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fp_store_data.close()"
      ],
      "metadata": {
        "id": "QOKI6P-56l5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if sent_pair_count == 0:\n",
        "#   sent_num = 23\n",
        "#   num_hing_sent = 8\n",
        "#   store(\"sentence_pair 3\")\n",
        "#   groundtruth = [\n",
        "#    [(0,0),(1,1),(2,2),(3,3),(4,4),(5,6)],\n",
        "#    [(0,2),(1,0),(2,6),(3,7),(4,8),(5,9)],\n",
        "#    [(0,2),(1,0),(2,6),(3,7),(4,8),(5,10)],\n",
        "#    [(0,0),(1,1),(2,2),(3,3),(4,4),(5,6)],\n",
        "#    [(0,0),(1,1),(2,2),(3,3),(4,4),(5,6)],\n",
        "#    [(0,2),(1,0),(2,6),(3,7),(4,8),(5,9)],\n",
        "#    [(0,0),(1,1),(2,2),(3,3),(4,4),(5,6)],\n",
        "#    [(0,2),(1,0),(2,6),(3,7),(4,8),(5,10)]\n",
        "#   ]\n",
        "# elif sent_pair_count == 1:\n",
        "#   sent_num = 35\n",
        "#   num_hing_sent = 8\n",
        "#   store(\"sentence_pair 4\")\n",
        "#   groundtruth = [\n",
        "#     [(0,4),(1,1),(2,2),(3,6)],\n",
        "#     [(0,3),(1,1),(2,2),(3,5)],\n",
        "#     [(0,0),(1,1),(2,2),(3,3)],\n",
        "#     [(0,5),(1,2),(2,0),(3,7)],\n",
        "#     [(0,5),(1,3),(2,1),(3,7)],\n",
        "#     [(0,0),(1,3),(2,1),(3,4)],\n",
        "#     [(0,0),(1,1),(2,2),(3,3)],\n",
        "#     [(0,4),(1,2),(2,0),(3,5)]\n",
        "#   ]\n",
        "# elif sent_pair_count == 2:\n",
        "#   sent_num = 56\n",
        "#   num_hing_sent = 7\n",
        "#   store(\"sentence_pair 6\")\n",
        "#   groundtruth = [\n",
        "#     [(0,5),(1,2),(2,0),(3,1)],\n",
        "#     [(0,0),(1,1),(3,2)],\n",
        "#     [(0,4),(1,1),(3,0)],\n",
        "#     [(0,4),(1,2),(3,0)],\n",
        "#     [(0,0),(1,1),(3,2)],\n",
        "#     [(0,4),(1,2),(2,0),(3,1),(4,5)],\n",
        "#     [(0,3),(1,1),(3,0),(4,4)]\n",
        "#   ]\n",
        "# elif sent_pair_count == 3:\n",
        "#   sent_num = 77\n",
        "#   num_hing_sent = 6\n",
        "#   store(\"sentence_pair 8\")\n",
        "#   groundtruth = [\n",
        "#     [(0,3),(1,4),(2,1),(3,2)],\n",
        "#     [(0,2),(1,3),(2,0),(3,1)],\n",
        "#     [(0,2),(1,3),(2,0),(3,1)],\n",
        "#     [(0,3),(1,4),(2,1),(3,2)],\n",
        "#     [(0,0),(1,1),(2,2),(3,3)],\n",
        "#     [(0,0),(1,1),(2,3),(3,4)],\n",
        "#   ]"
      ],
      "metadata": {
        "id": "M_910GC-aA55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# store(\"[eng] \"+ data[sent_num]) \n",
        "# eng_sent = word_tokenize(data[sent_num])\n",
        "# sent_num += 2\n",
        "# for i in range(num_hing_sent):\n",
        "#   store(\"[hing] \"+data[sent_num])\n",
        "#   #tokenize\n",
        "#   store(\"[nltk_tokenize]\")\n",
        "#   store(\"[eng] \"+ str(eng_sent))\n",
        "#   hing_sent = word_tokenize(data[sent_num])\n",
        "#   store(\"[hing] \"+ str(hing_sent))\n",
        "#   eng_sent = [w for w in eng_sent if not w.lower() in stop_words]\n",
        "#   hing_sent = [w for w in hing_sent if not w.lower() in stop_words]\n",
        "#   store(\"after stopwords removal\")\n",
        "#   store(\"[eng]\"+str(eng_sent))\n",
        "#   store(\"[hing]\"+str(hing_sent))\n",
        "#   store(\"enumerate tokens\")\n",
        "#   store(\"[eng]\"+str(list(enumerate(eng_sent))))\n",
        "#   store(\"[hing]\"+str(list(enumerate(hing_sent))))\n",
        "#   store(\"groundtruth\")\n",
        "#   store(str(groundtruth[i]))\n",
        "#   alignments_simalign = myaligner.get_word_aligns(eng_sent, hing_sent)\n",
        "#   store_alignments = {}\n",
        "#   for matching_method in alignments_simalign:\n",
        "#     store_alignments[matching_method] = alignments_simalign[matching_method]\n",
        "#   store(\"simalign output\")\n",
        "#   store(str(store_alignments))\n",
        "\n",
        "#   ################\n",
        "#   methods_of_simalign = [\"mwmf\",\"inter\",\"itermax\",\"fwd\",\"rev\"]\n",
        "#   for j in range(len(methods_of_simalign)):\n",
        "#     simalign_method = methods_of_simalign[j]\n",
        "#     count = 0\n",
        "#     simalign_map = []\n",
        "#     for item in alignments_simalign[simalign_method]:\n",
        "#       temp = str(eng_sent[item[0]]) + \"----->\" + str(hing_sent[item[1]])\n",
        "#       simalign_map.append(temp)\n",
        "#       if item in groundtruth[i]:\n",
        "#         count += 1\n",
        "#     # accuracy \n",
        "#     accuracy = round(count/len(alignments_simalign[simalign_method]),2)\n",
        "#     store(\"method: \"+simalign_method)\n",
        "#     store(\"accuracy: \"+str(accuracy))\n",
        "#     store(str(simalign_map))\n",
        "#   sent_num += 1\n",
        "#   store(\"\\n\")\n",
        "# sent_pair_count += 1"
      ],
      "metadata": {
        "id": "pbKZjaVaLJlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fp_store_data.close()"
      ],
      "metadata": {
        "id": "AMZguxJPRu4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sent_pair_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_7nYXSna0jf",
        "outputId": "5dfb9b3f-194f-417d-b2b1-adee65ea5ee1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Pair 3 \n",
        "# nltk tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "eng_sent = word_tokenize(data[23])\n",
        "source_eng = data[23]\n",
        "if count_hing_sent == 0:\n",
        "  hing_sent = word_tokenize(data[25])\n",
        "  source_hing = data[25]\n",
        "  groundtruth = [(0,0),(1,1),(2,2),(3,3),(4,4),(5,6)]\n",
        "elif count_hing_sent == 1:\n",
        "  hing_sent = word_tokenize(data[26])\n",
        "  source_hing = data[26]\n",
        "elif count_hing_sent == 2:\n",
        "  hing_sent = word_tokenize(data[27])\n",
        "  source_hing = data[27]\n",
        "elif count_hing_sent == 3:\n",
        "  hing_sent = word_tokenize(data[28])\n",
        "  source_hing = data[28]\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "hing_sent = word_tokenize(data[27])\n",
        "source_hing = data[27]\n",
        "#hing_sent = word_tokenize(data[28])\n",
        "#source_hing = data[28]\n",
        "#hing_sent = word_tokenize(data[29])\n",
        "#source_hing = data[29]\n",
        "#hing_sent = word_tokenize(data[30])\n",
        "#source_hing = data[30]\n",
        "#hing_sent = word_tokenize(data[31])\n",
        "#source_hing = data[31]\n",
        "#hing_sent = word_tokenize(data[32])\n",
        "#source_hing = data[32]"
      ],
      "metadata": {
        "id": "2k9UNkbYLQJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Pair 4\n",
        "# nltk tokenize\n",
        "#from nltk.tokenize import word_tokenize\n",
        "#eng_sent = word_tokenize(data[35])\n",
        "#hing_sent = word_tokenize(data[37])\n",
        "#hing_sent = word_tokenize(data[38])\n",
        "#hing_sent = word_tokenize(data[39])\n",
        "#hing_sent = word_tokenize(data[40])\n",
        "#hing_sent = word_tokenize(data[41])\n",
        "#hing_sent = word_tokenize(data[42])\n",
        "#hing_sent = word_tokenize(data[43])\n",
        "#hing_sent = word_tokenize(data[44])"
      ],
      "metadata": {
        "id": "XjgrOGLShO9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Pair 6 - hing\n",
        "# nltk tokenize\n",
        "#from nltk.tokenize import word_tokenize\n",
        "#eng_sent = word_tokenize(data[56])\n",
        "#hing_sent = word_tokenize(data[58])\n",
        "#hing_sent = word_tokenize(data[59])\n",
        "#hing_sent = word_tokenize(data[60]) # failed\n",
        "#hing_sent = word_tokenize(data[61]) # failed\n",
        "#hing_sent = word_tokenize(data[62])\n",
        "#hing_sent = word_tokenize(data[63])\n",
        "#hing_sent = word_tokenize(data[64]) # failed"
      ],
      "metadata": {
        "id": "wNpU2xSj_7sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sentence pair 6 - hindi\n",
        "#from nltk.tokenize import word_tokenize\n",
        "#eng_sent = word_tokenize(data[56])\n",
        "#hing_sent = word_tokenize(data[57])"
      ],
      "metadata": {
        "id": "isQOg7kUkquI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sentence pair 8 - hindi\n",
        "#from nltk.tokenize import word_tokenize\n",
        "#eng_sent = word_tokenize(data[77])\n",
        "#hing_sent = word_tokenize(data[78])"
      ],
      "metadata": {
        "id": "wSqTnADGl0rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sentence pair 10\n",
        "#from nltk.tokenize import word_tokenize\n",
        "#eng_sent = word_tokenize(data[97])\n",
        "#hing_sent = word_tokenize(data[99]) # giving wrong results  using xlm-roberta-base"
      ],
      "metadata": {
        "id": "r6riW2stnAhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# store_data\n",
        "store(\"[eng]\"+source_eng)\n",
        "store(\"[hing]\"+source_hing)\n",
        "store(\"[nltk_tokenize]\")\n",
        "store(\"[eng]\"+str(eng_sent))\n",
        "store(\"[hing]\"+str(hing_sent))"
      ],
      "metadata": {
        "id": "Fve5OC9thnQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "eng_sent = [w for w in eng_sent if not w.lower() in stop_words]\n",
        "hing_sent = [w for w in hing_sent if not w.lower() in stop_words]"
      ],
      "metadata": {
        "id": "O27i8t8PWoad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# after stop words removal\n",
        "print(eng_sent)\n",
        "print(hing_sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jYkBGBRWclL",
        "outputId": "694b293c-69b0-4b44-d80f-56dcb46c2515"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Connecting', 'server', ',', 'please', 'wait', '...']\n",
            "['Server', 'se', 'connect', 'kar', 'rhe', 'hai', ',', 'please', 'intezaar', 'karein', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# store\n",
        "store(\"after stopwords removal\")\n",
        "store(\"[eng]\"+str(eng_sent))\n",
        "store(\"[hing]\"+str(hing_sent))"
      ],
      "metadata": {
        "id": "L3GWz05jqcEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(enumerate(eng_sent)))\n",
        "print(list(enumerate(hing_sent)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQm01InnXCnl",
        "outputId": "20f71906-bb75-49cb-9381-626a3cf85bcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 'Connecting'), (1, 'server'), (2, ','), (3, 'please'), (4, 'wait'), (5, '...')]\n",
            "[(0, 'Server'), (1, 'se'), (2, 'connect'), (3, 'kar'), (4, 'rhe'), (5, 'hai'), (6, ','), (7, 'please'), (8, 'intezaar'), (9, 'karein'), (10, '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# store\n",
        "store(\"enumerate tokens\")\n",
        "store(\"[eng]\"+str(list(enumerate(eng_sent))))\n",
        "store(\"[hing]\"+str(list(enumerate(hing_sent))))"
      ],
      "metadata": {
        "id": "vaTorxierBaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Pair 3 \n",
        "\n",
        "# 25\n",
        "#groundtruth = [(0,0),(1,1),(2,2),(3,3),(4,4),(5,6)]\n",
        "# 26\n",
        "#groundtruth = [(0,2),(1,0),(2,6),(3,7),(4,8),(5,9)]\n",
        "# 27\n",
        "#groundtruth = [(0,2),(1,0),(2,6),(3,7),(4,8),(5,10)]\n",
        "# 28\n",
        "groundtruth = [(0,0),(1,1),(2,2),(3,3),(4,4),(5,6)]\n",
        "# 29\n",
        "#groundtruth = [(0,0),(1,1),(2,2),(3,3),(4,4),(5,6)]\n",
        "# 30\n",
        "#groundtruth = [(0,2),(1,0),(2,6),(3,7),(4,8),(5,9)]\n",
        "# 31 \n",
        "#groundtruth = [(0,0),(1,1),(2,2),(3,3),(4,4),(5,6)]\n",
        "# 32\n",
        "#groundtruth = [(0,2),(1,0),(2,6),(3,7),(4,8),(5,10)]\n"
      ],
      "metadata": {
        "id": "3SCAAa9FQcR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Pair 4\n",
        "# 37\n",
        "#groundtruth = \n",
        "# 38\n",
        "#groundtruth = \n",
        "# 39\n",
        "#groundtruth = \n",
        "# 40\n",
        "#groundtruth = \n",
        "# 41\n",
        "#groundtruth = \n",
        "# 42\n",
        "#groundtruth = \n",
        "# 43\n",
        "#groundtruth = \n",
        "# 44\n",
        "#groundtruth = \n"
      ],
      "metadata": {
        "id": "5znA1SgPhh67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Pair 6 - hing\n",
        "# 58\n",
        "#groundtruth = \n",
        "# 59\n",
        "#groundtruth = \n",
        "# 60\n",
        "#groundtruth = \n",
        "# 61\n",
        "#groundtruth = \n",
        "# 62\n",
        "#groundtruth = \n",
        "# 63\n",
        "#groundtruth = \n",
        "# 64\n",
        "#groundtruth = \n"
      ],
      "metadata": {
        "id": "RiYReJUIARBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Pair 6 hindi\n",
        "#groundtruth = [(0,5),(1,2),(3,0),(4,6)]"
      ],
      "metadata": {
        "id": "DIqOB-NPlB5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Pair 8 hindi\n",
        "#groundtruth = [(0,0),(1,1),(2,3),(3,1)]"
      ],
      "metadata": {
        "id": "xkgmAWSxl-S5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Pair 10  \n",
        "# 99\n",
        "#groundtruth = [(0,4),(1,7),(2,6),(3,8)]"
      ],
      "metadata": {
        "id": "Heo_TAXAnaJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store(\"groundtruth\")\n",
        "store(str(groundtruth))"
      ],
      "metadata": {
        "id": "sYhInm1grdK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alignments_simalign = myaligner.get_word_aligns(eng_sent, hing_sent)"
      ],
      "metadata": {
        "id": "26xzWrhkL2pm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in list(enumerate(eng_sent)):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrcqB-rXMT8R",
        "outputId": "f601e9e8-dff1-42bd-b446-ad9f437b2718"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, 'Connecting')\n",
            "(1, 'server')\n",
            "(2, ',')\n",
            "(3, 'please')\n",
            "(4, 'wait')\n",
            "(5, '...')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in list(enumerate(hing_sent)):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnjfJiyPMjx6",
        "outputId": "73e08b69-d8e7-46a9-df3b-469c9a9be70f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, 'Server')\n",
            "(1, 'se')\n",
            "(2, 'connect')\n",
            "(3, 'kar')\n",
            "(4, 'rhe')\n",
            "(5, 'hai')\n",
            "(6, ',')\n",
            "(7, 'please')\n",
            "(8, 'intezaar')\n",
            "(9, 'karein')\n",
            "(10, '.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "store_alignments = {}\n",
        "for matching_method in alignments_simalign:\n",
        "    print(matching_method, \":\", alignments_simalign[matching_method])\n",
        "    store_alignments[matching_method] = alignments_simalign[matching_method]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvOQ9SXOMqqD",
        "outputId": "1efc39d6-ba5a-4f07-9815-e5ee09291bbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mwmf : [(0, 1), (0, 2), (0, 3), (1, 0), (2, 6), (3, 7), (4, 8), (5, 4), (5, 5), (5, 10)]\n",
            "inter : [(0, 2), (1, 0), (2, 6), (3, 7), (5, 10)]\n",
            "itermax : [(0, 1), (0, 2), (1, 0), (2, 6), (3, 7), (4, 8), (5, 10)]\n",
            "fwd : [(0, 0), (0, 2), (1, 0), (2, 6), (3, 7), (4, 2), (5, 10)]\n",
            "rev : [(0, 1), (0, 2), (0, 3), (0, 4), (0, 8), (0, 9), (1, 0), (1, 5), (1, 9), (2, 6), (3, 7), (4, 8), (5, 10)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "store(\"simalign output\")\n",
        "store(str(store_alignments))"
      ],
      "metadata": {
        "id": "r24RxGuisFSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "methods_of_simalign = [\"mwmf\",\"inter\",\"itermax\",\"fwd\",\"rev\"]\n",
        "for i in range(len(methods_of_simalign)):\n",
        "  simalign_method = methods_of_simalign[i]\n",
        "  count = 0\n",
        "  for item in alignments_simalign[simalign_method]:\n",
        "    temp = str(eng_sent[item[0]]) + \"----->\" + str(hing_sent[item[1]])\n",
        "    simalign_map.append(temp)\n",
        "    if item in groundtruth:\n",
        "      count += 1\n",
        "  # accuracy \n",
        "  accuracy = round(count/len(alignments_simalign[simalign_method]),2)\n",
        "  store(\"method: \"+simalign_method)\n",
        "  store(\"accuracy: \"+str(accuracy))\n",
        "  store(str(simalign_map))\n",
        "store(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "CxSw3xZNE6kX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"Alignment [groundtruth] and [simAlign]\")\n",
        "# print(\"Word in Sent 1 -----> Word in Sent 2\")\n",
        "# sent1 = []\n",
        "# sent2 = []\n",
        "# count = 0\n",
        "# methods_of_simalign = [\"mwmf\",\"inter\",\"itermax\",\"fwd\",\"rev\"]\n",
        "# simalign_method = methods_of_simalign[method_track % len(methods_of_simalign)]\n",
        "# alignments = alignments_simalign[simalign_method]\n",
        "# method_track+=1\n",
        "# print(\"\\nsimAlign\")\n",
        "# print(\"method: \",simalign_method)\n",
        "# simalign_map = []\n",
        "# for item in alignments:\n",
        "#   print(eng_sent[item[0]],\"---------->\",hing_sent[item[1]])\n",
        "#   temp = str(eng_sent[item[0]]) + \"----->\" + str(hing_sent[item[1]])\n",
        "#   simalign_map.append(temp)\n",
        "#   sent1.append(eng_sent[item[0]])\n",
        "#   sent2.append(hing_sent[item[1]])\n",
        "#   if item in groundtruth:\n",
        "#     count += 1\n",
        "# #print(\"\\ngroundtruth\")\n",
        "# for item in groundtruth:\n",
        "#   print(eng_sent[item[0]],\"---------->\",hing_sent[item[1]])\n",
        "# # accuracy \n",
        "# accuracy = round(count/len(alignments),2)\n",
        "# print(f\"accuracy: {accuracy}\")\n",
        "# store(\"method: \"+simalign_method)\n",
        "# store(\"accuracy: \"+str(accuracy))\n",
        "# store(str(simalign_map))"
      ],
      "metadata": {
        "id": "z_OiX9nFMtHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# store(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "26HtM_BDyAYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp_store_data.close()"
      ],
      "metadata": {
        "id": "na0UXBp3w9J9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "from sklearn import metrics\n",
        "\n",
        "actual = [0,1,1,0]\n",
        "predicted = [1,0,1,0]\n",
        "\n",
        "confusion_matrix = metrics.confusion_matrix(actual, predicted)\n",
        "\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [False, True])\n",
        "\n",
        "cm_display.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "HnRcPTqjIwSq",
        "outputId": "7856ce3f-5016-4710-8eb5-c7ff5cd42dee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  for j in range(3):\n",
        "    if j == 1:\n",
        "      continue\n",
        "    print(i,j,\"Salam\")"
      ],
      "metadata": {
        "id": "0mQK-t5K9VIK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4316ecb9-1e97-4784-d410-aaaca78f469a"
      },
      "execution_count": 458,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0 Salam\n",
            "0 2 Salam\n",
            "1 0 Salam\n",
            "1 2 Salam\n",
            "2 0 Salam\n",
            "2 2 Salam\n",
            "3 0 Salam\n",
            "3 2 Salam\n",
            "4 0 Salam\n",
            "4 2 Salam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tSwgT9J7A8xE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}